{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdecc145",
   "metadata": {
    "id": "cdecc145"
   },
   "source": [
    "# Fine-Tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa12603",
   "metadata": {
    "id": "bfa12603",
    "outputId": "f7a52a58-e130-4d3d-f403-baec811b99de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\p4k\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49de1a1",
   "metadata": {
    "id": "f49de1a1"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f5d14",
   "metadata": {
    "id": "583f5d14"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc8873",
   "metadata": {
    "id": "82bc8873",
    "outputId": "bec25623-f9eb-471f-b5d7-f1f2bf8db2b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0</th>\n",
       "      <th>9411</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/12589-johnny-cash-remixed/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>I've been scouring the book of Revelation for some mention of this album, figuring it had to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9832</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/12135-angles/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>If you're an American reader familiar with the British hip-hop duo Dan Le Sac vs. Scroobius Pip,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9461</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/12544-the-effects-of-333/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>Credit where credit's due: Black Rebel Motorcycle Club have made far better music than either th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>3914</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/18560-pixies-ep-1/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>The sad spoils of a job in music criticism: I am finally given the chance to review a new releas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/15996-lou-reed-metallica/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>When Metallica announced last June that they had recorded a new album with Lou Reed, fans of bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10.0</th>\n",
       "      <th>7004</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/15069-nowhere-20th-anniversary-edition/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Along with the 20th anniversary reissue of Ride's debut LP, Nowhere, comes a thick booklet of ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/21845-sign-o-the-times/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>In 1987, Prince Rogers Nelson was in transition. He’d disbanded the Revolution, the band that ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8835</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/13130-reckoning-deluxe-edition/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>Given their vast and varied catalog, it's sometimes easier to imagine R.E.M. as a discography th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4639</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/17499-rumours/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>Fleetwood Mac's Rumours would never be just an album. Upon its release in 1977, it became the fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>http://pitchfork.com/reviews/albums/22061-another-green-world/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>In July 1975, Brian Eno found himself a few days and several thousand dollars into a studio book...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3071 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    url  \\\n",
       "score                                                                                     \n",
       "0.0   9411               http://pitchfork.com/reviews/albums/12589-johnny-cash-remixed/   \n",
       "      9832                            http://pitchfork.com/reviews/albums/12135-angles/   \n",
       "      9461                http://pitchfork.com/reviews/albums/12544-the-effects-of-333/   \n",
       "1.0   3914                       http://pitchfork.com/reviews/albums/18560-pixies-ep-1/   \n",
       "      6094                http://pitchfork.com/reviews/albums/15996-lou-reed-metallica/   \n",
       "...                                                                                 ...   \n",
       "10.0  7004  http://pitchfork.com/reviews/albums/15069-nowhere-20th-anniversary-edition/   \n",
       "      857                   http://pitchfork.com/reviews/albums/21845-sign-o-the-times/   \n",
       "      8835          http://pitchfork.com/reviews/albums/13130-reckoning-deluxe-edition/   \n",
       "      4639                           http://pitchfork.com/reviews/albums/17499-rumours/   \n",
       "      355                http://pitchfork.com/reviews/albums/22061-another-green-world/   \n",
       "\n",
       "            score  pub_year  \\\n",
       "score                         \n",
       "0.0   9411    0.0      2009   \n",
       "      9832    0.0      2008   \n",
       "      9461    0.0      2009   \n",
       "1.0   3914    1.0      2013   \n",
       "      6094    1.0      2011   \n",
       "...           ...       ...   \n",
       "10.0  7004   10.0      2011   \n",
       "      857    10.0      2016   \n",
       "      8835   10.0      2009   \n",
       "      4639   10.0      2013   \n",
       "      355    10.0      2016   \n",
       "\n",
       "                                                                                                           text  \n",
       "score                                                                                                            \n",
       "0.0   9411  I've been scouring the book of Revelation for some mention of this album, figuring it had to be ...  \n",
       "      9832  If you're an American reader familiar with the British hip-hop duo Dan Le Sac vs. Scroobius Pip,...  \n",
       "      9461  Credit where credit's due: Black Rebel Motorcycle Club have made far better music than either th...  \n",
       "1.0   3914  The sad spoils of a job in music criticism: I am finally given the chance to review a new releas...  \n",
       "      6094  When Metallica announced last June that they had recorded a new album with Lou Reed, fans of bot...  \n",
       "...                                                                                                         ...  \n",
       "10.0  7004  Along with the 20th anniversary reissue of Ride's debut LP, Nowhere, comes a thick booklet of ol...  \n",
       "      857   In 1987, Prince Rogers Nelson was in transition. He’d disbanded the Revolution, the band that ha...  \n",
       "      8835  Given their vast and varied catalog, it's sometimes easier to imagine R.E.M. as a discography th...  \n",
       "      4639  Fleetwood Mac's Rumours would never be just an album. Upon its release in 1977, it became the fa...  \n",
       "      355   In July 1975, Brian Eno found himself a few days and several thousand dollars into a studio book...  \n",
       "\n",
       "[3071 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('p4k_reviews_dataset_processed_balanced.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c67675",
   "metadata": {
    "id": "92c67675",
    "outputId": "b2103f1d-ca33-4783-ca51-5b8fa61981b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(score      \n",
       " 0.0    9411    I've been scouring the book of Revelation for some mention of this album, figuring it had to be ...\n",
       "        9832    If you're an American reader familiar with the British hip-hop duo Dan Le Sac vs. Scroobius Pip,...\n",
       "        9461    Credit where credit's due: Black Rebel Motorcycle Club have made far better music than either th...\n",
       " 1.0    3914    The sad spoils of a job in music criticism: I am finally given the chance to review a new releas...\n",
       "        6094    When Metallica announced last June that they had recorded a new album with Lou Reed, fans of bot...\n",
       " Name: text, dtype: object,\n",
       " score      \n",
       " 0.0    9411    0.0\n",
       "        9832    0.0\n",
       "        9461    0.0\n",
       " 1.0    3914    1.0\n",
       "        6094    1.0\n",
       " Name: score, dtype: float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = df['text']\n",
    "dataY = df['score']\n",
    "dataX.head(5), dataY.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b9abb",
   "metadata": {
    "id": "495b9abb"
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.80\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataX, dataY, test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715aa1e",
   "metadata": {
    "id": "c715aa1e"
   },
   "outputs": [],
   "source": [
    "raw_train_df = pd.concat([x_train, y_train], axis=1)\n",
    "raw_test_df = pd.concat([x_test, y_test], axis=1)\n",
    "raw_val_df = pd.concat([x_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc97b1",
   "metadata": {
    "id": "64dc97b1"
   },
   "outputs": [],
   "source": [
    "raw_train_ds = Dataset.from_pandas(raw_train_df)\n",
    "raw_test_ds = Dataset.from_pandas(raw_test_df)\n",
    "raw_val_ds = Dataset.from_pandas(raw_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630dbb0",
   "metadata": {
    "id": "a630dbb0",
    "outputId": "5b49d4c6-0a38-4d7c-811b-42bde008ecff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'score', '__index_level_0__', '__index_level_1__'],\n",
      "    num_rows: 2456\n",
      "}) Dataset({\n",
      "    features: ['text', 'score', '__index_level_0__', '__index_level_1__'],\n",
      "    num_rows: 308\n",
      "}) Dataset({\n",
      "    features: ['text', 'score', '__index_level_0__', '__index_level_1__'],\n",
      "    num_rows: 308\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_ds, raw_val_ds, raw_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cf126",
   "metadata": {
    "id": "111cf126",
    "outputId": "533efb33-c5e9-4978-b0f0-6aac7738e05e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"bert-base-uncased\"\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 12\n",
    "EPOCHS = 20\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824ef6f",
   "metadata": {
    "id": "4824ef6f",
    "outputId": "5d0b310f-6237-44dc-eb16-42843b7471e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "ds = {\"train\": raw_train_ds, \"validation\": raw_val_ds, \"test\": raw_test_ds}\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    label = examples[\"score\"]\n",
    "    examples = tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    # Change this to real number\n",
    "    examples[\"label\"] = float(label)\n",
    "    return examples\n",
    "\n",
    "for split in ds:\n",
    "    ds[split] = ds[split].map(preprocess_function, remove_columns=[\"text\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e908850",
   "metadata": {
    "id": "8e908850"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n",
    "\n",
    "    # Compute accuracy\n",
    "    # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5\n",
    "    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n",
    "\n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0dba08",
   "metadata": {
    "id": "2c0dba08"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/pitchfork-bert\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7658a8",
   "metadata": {
    "id": "ea7658a8"
   },
   "outputs": [],
   "source": [
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0][:, 0]\n",
    "        loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85881b73",
   "metadata": {
    "id": "85881b73",
    "outputId": "0e181f39-03e5-4920-86c6-fcc124a8c51d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\anaconda3\\envs\\p4k\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2456\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4100\n",
      "  Number of trainable parameters = 109483009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4100' max='4100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4100/4100 59:08, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.159673</td>\n",
       "      <td>2.159673</td>\n",
       "      <td>1.159783</td>\n",
       "      <td>0.318851</td>\n",
       "      <td>0.253247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.743036</td>\n",
       "      <td>1.743036</td>\n",
       "      <td>1.014747</td>\n",
       "      <td>0.450256</td>\n",
       "      <td>0.311688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.727400</td>\n",
       "      <td>2.043428</td>\n",
       "      <td>2.043428</td>\n",
       "      <td>1.096771</td>\n",
       "      <td>0.355514</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.727400</td>\n",
       "      <td>1.731110</td>\n",
       "      <td>1.731110</td>\n",
       "      <td>1.013685</td>\n",
       "      <td>0.454018</td>\n",
       "      <td>0.347403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>1.637751</td>\n",
       "      <td>1.637751</td>\n",
       "      <td>0.957066</td>\n",
       "      <td>0.483462</td>\n",
       "      <td>0.399351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>1.535734</td>\n",
       "      <td>1.535734</td>\n",
       "      <td>0.929292</td>\n",
       "      <td>0.515638</td>\n",
       "      <td>0.399351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>1.509741</td>\n",
       "      <td>1.509741</td>\n",
       "      <td>0.916320</td>\n",
       "      <td>0.523836</td>\n",
       "      <td>0.396104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>1.460881</td>\n",
       "      <td>1.460881</td>\n",
       "      <td>0.921639</td>\n",
       "      <td>0.539246</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>1.510837</td>\n",
       "      <td>1.510837</td>\n",
       "      <td>0.911221</td>\n",
       "      <td>0.543490</td>\n",
       "      <td>0.389610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>1.426096</td>\n",
       "      <td>1.426096</td>\n",
       "      <td>0.911429</td>\n",
       "      <td>0.558678</td>\n",
       "      <td>0.402597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>1.357936</td>\n",
       "      <td>1.357936</td>\n",
       "      <td>0.916938</td>\n",
       "      <td>0.578636</td>\n",
       "      <td>0.402597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>1.208760</td>\n",
       "      <td>1.208760</td>\n",
       "      <td>0.916657</td>\n",
       "      <td>0.594146</td>\n",
       "      <td>0.402597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>1.210212</td>\n",
       "      <td>1.210212</td>\n",
       "      <td>0.924592</td>\n",
       "      <td>0.613688</td>\n",
       "      <td>0.429870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>1.227287</td>\n",
       "      <td>1.227287</td>\n",
       "      <td>0.936289</td>\n",
       "      <td>0.626763</td>\n",
       "      <td>0.463647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>1.181060</td>\n",
       "      <td>1.181059</td>\n",
       "      <td>0.804337</td>\n",
       "      <td>0.642882</td>\n",
       "      <td>0.446104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>1.115995</td>\n",
       "      <td>1.115995</td>\n",
       "      <td>0.806915</td>\n",
       "      <td>0.661864</td>\n",
       "      <td>0.499351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>1.176775</td>\n",
       "      <td>1.176775</td>\n",
       "      <td>0.770453</td>\n",
       "      <td>0.674233</td>\n",
       "      <td>0.499351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>1.199503</td>\n",
       "      <td>1.199503</td>\n",
       "      <td>0.778605</td>\n",
       "      <td>0.717065</td>\n",
       "      <td>0.495844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>1.190036</td>\n",
       "      <td>1.190036</td>\n",
       "      <td>0.770066</td>\n",
       "      <td>0.730051</td>\n",
       "      <td>0.492597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>1.161307</td>\n",
       "      <td>1.161307</td>\n",
       "      <td>0.800228</td>\n",
       "      <td>0.712804</td>\n",
       "      <td>0.492857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-205\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-205\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-205\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-2149] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-410\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-410\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-410\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-3070] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-615\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-615\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-615\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-205] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-820\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-820\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-820\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-410] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1025\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1025\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1025\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-615] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1230\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1230\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1230\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-820] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1435\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1435\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1435\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-1230] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1640\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1640\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1640\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-1435] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1845\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1845\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-1845\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-1640] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2050\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2050\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2050\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-1025] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2255\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2255\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2255\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-1845] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2460\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2460\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2460\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-2255] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2665\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2665\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2665\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-2460] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2870\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2870\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-2870\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-2665] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3075\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3075\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3075\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-2870] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3280\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3280\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3280\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-3075] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3485\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3485\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3485\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-3280] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3690\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3690\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3690\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-2050] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3895\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3895\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3895\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-3485] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-4100\n",
      "Configuration saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-4100\\config.json\n",
      "Model weights saved in ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-4100\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\bert-base-uncased-fine-tuned-regression-2\\checkpoint-3895] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models/bert-base-uncased-fine-tuned-regression-2\\checkpoint-3690 (score: 0.40584415584415584).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4100, training_loss=0.6366599191107402, metrics={'train_runtime': 3551.6947, 'train_samples_per_second': 13.83, 'train_steps_per_second': 1.154, 'total_flos': 1.292389899976704e+16, 'train_loss': 0.6366599191107402, 'epoch': 20.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = RegressionTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    compute_metrics=compute_metrics_for_regression,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5e610",
   "metadata": {
    "id": "8ef5e610",
    "outputId": "c244087d-9184-4319-e945-f146a36408ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, __index_level_1__. If __index_level_0__, __index_level_1__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 308\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.99132846431497314,\n",
       " 'eval_mse': 1.2365033740997314,\n",
       " 'eval_mae': 0.8236053466796875,\n",
       " 'eval_r2': 0.7110650978080474,\n",
       " 'eval_accuracy': 0.40584415584415584,\n",
       " 'eval_runtime': 7.3482,\n",
       " 'eval_samples_per_second': 41.915,\n",
       " 'eval_steps_per_second': 3.538,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_dataset=ds[\"test\"]\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846737d9",
   "metadata": {
    "id": "846737d9"
   },
   "outputs": [],
   "source": [
    "nb_batches = math.ceil(len(raw_test_ds)/BATCH_SIZE)\n",
    "y_preds = []\n",
    "\n",
    "for i in range(nb_batches):\n",
    "    input_texts = raw_test_ds[i * BATCH_SIZE: (i+1) * BATCH_SIZE][\"text\"]\n",
    "    input_labels = raw_test_ds[i * BATCH_SIZE: (i+1) * BATCH_SIZE][\"score\"]\n",
    "    encoded = tokenizer(input_texts, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_tensors=\"pt\").to(\"cuda\")\n",
    "    y_preds += model(**encoded).logits.reshape(-1).tolist()\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "df1 = pd.DataFrame([raw_test_ds[\"text\"], raw_test_ds[\"score\"], y_preds], [\"Text\", \"Score\", \"Prediction\"]).T\n",
    "# incorrect_cases = df[df[\"Score\"] != df[\"Rounded Prediction\"]]\n",
    "# incorrect_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d9eb4",
   "metadata": {
    "id": "9e4d9eb4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
